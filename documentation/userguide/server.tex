\documentclass[notoc,justified,openany]{tufte-book}    % `notoc` suppresses TL custom TOC, reverts to standard LaTeX
\usepackage{graphicx}
\usepackage{xspace}
\hyphenation{HMMER}
\newcommand{\hmmserver}{\mono{hmmserver}\xspace}
\newcommand{\Hmmserver}{\mono{Hmmserver}\xspace}
\newcommand{\hmmclient}{\mono{hmmclient}\xspace}
\newcommand{\Hmmclient}{\mono{Hmmclient}\xspace}
\newcommand{\hmmpgmd}{\mono{hmmpgmd}\xspace}
\newcommand{\Hmmpgmd}{\mono{Hmmpgmd}\xspace}
\newcommand{\userguide}{HMMER User's Guide}
\input{titlepage_server}                    % definitions for \maketitle 
\bibliographystyle{unsrtnat-brief}   % customized natbib unsrtnat. Abbrev 3+ authors to ``et al.'' 

\begin{document}
\setcounter{tocdepth}{2}             % 0=chapters 1=sections 2=subsections 3=subsubsections? 4=paragraphs
\input{inclusions/inclusions.def}    % snippets captured from output, by gen-inclusions.py 

\maketitle

\input{copyright}

\begin{adjustwidth}{}{-1in}          % TL \textwidth is quite narrow. Expand it manually for TOC and man pages.
\tableofcontents                     
\end{adjustwidth}


\chapter{Introduction}
\Hmmserver and \hmmclient are replacements for the \hmmpgmd and \mono{hmmc2} programs provided by earlier versions of HMMER.  Like \hmmpgmd, \hmmserver is a persistent, long-running, service that provides high-performance homology searches by caching sequence and HMM databases in RAM and distributing the work of each search across many computers and threads.  \Hmmclient is a full-featured command-line client application for \hmmserver that submits searches to a running server and displays results in a format that is as close as possible to that of the \mono{hmmsearch}, \mono{hmmscan}, \mono{phmmer}, and \mono{jackhmmer} programs.  It is thus a significant upgrade to the \mono{hmmc2} program, which was more of a debugging tool for \hmmpgmd than a full client program.

\Hmmserver improves on \hmmpgmd in a number of ways:
\begin{enumerate}
  \item{A single \hmmserver instance can support searches of multiple databases\sidenote{"Database" is used here to describe a collection of sequence or HMM data that can be searched, which is typically implemented as a single file of data, as opposed to an SQL database or some similar structure.} of sequence and/or HMM data by loading multiple data files into RAM simultaneously.  In contrast, \hmmpgmd can only handle a single data file, requiring users to run multiple instances of \hmmpgmd or merge multiple databases into a single data file if they want to be able to search more than one database.}
  \item{\Hmmserver is implemented as an MPI application that can typically be invoked with a single command, as compared to \hmmpgmd, which required that users start an instance of its master program, wait for that to initialize, and then manually start worker instances on each worker machine.}
  \item{\Hmmserver implements dynamic allocation of work across the worker nodes to improve performance over \hmmpgmd, which allocated a fixed fraction of a search to each worker node, and uses a work-stealing algorithm within each worker node to further improve performance.  Together, these improvements in parallelization increase search performance by approximately 50\% when running on a large number of cores.}
  \item{\Hmmserver implements an improved scheme to reduce memory use by distributing target data across multiple machines (sharding).  \mono{Hmmpgmd} implemented sharding, but required that each worker node in an N-node system load $\frac{1}{Nth}$ of the data, which was reasonable given that \hmmpgmd allocated a fixed fraction of each search to each worker node.  In contrast, \hmmserver allows the user to specify the number of shards that each database should be broken into independently of the number of worker nodes\sidenote{As long as the server has at least as many worker nodes as shards.} to allow trade-offs between the amount of memory required on each worker node and performance.  
  
  For example, a server with 12 worker nodes could be configured with one shard to maximize performance by providing the most opportunity to dynamically allocate work to worker nodes, at the cost of requiring that each worker node load all of each database into RAM, with 12 shards to minimize memory usage at the cost of not allowing the server to dynamically allocate work to nodes, or with 4 shards, allowing the work of searching each shard to be dynamically allocated to the three nodes that load the shard's data.}
  \item{\Hmmserver reads standard FASTA or HMM files, while \hmmpgmd required that data files be pre-processed into a special format.  Among other things, this format discarded the metadata (name, accession, etc.) assocated with a sequence or HMM and replaced it with a numeric ID.  While this reduced the amount of memory required to hold the database, it required that the results of each search be post-processed to replace that ID with the appropriate metadata to create human-readable results.  Having to implement this post-processing step greatly increased the effort required to use \hmmpgmd.  In addition, post-processing of search results has become a major performance bottleneck for the server implemented by the European Bioinformatics Institute, which the switch to \hmmserver will eliminate.}
\end{enumerate}

\Hmmclient is the companion application to \hmmserver, and provides a command-line interface that users can use to send queries to a server.  \Hmmclient is designed to mimic as much as possible the behavior of HMMERs command-line search applications \mono{hmmsearch}, \mono{hmmscan}, \mono{phmmer}, and \mono{jackhmmer}.  In particular, it generates the same output formats as these command-line applications, allowing it to be a drop-in replacement for them in analysis pipelines.

\Hmmserver is most useful when a user wants to perform searches interactively, running one search, examining the results, and using them to identify a second search of interest.  While \hmmserver's performance scales very well with the number of cores allocated to the server\sidenote{Add example number when we have data}, it does not scale perfectly.  Thus, when a user needs to perform many searches of a given database, it may be more efficient to use our command-line search applications to perform multiple searches in parallel than to use \hmmserver, although this will depend on whether the file systems of the hardware those searches run on can meet the I/O demands of multiple simultaneous searches.  Users are advised to run pilot experiments on their own systems before doing large-scale runs.

The remainder of this manual begins with a discussion of how to install and use the \hmmserver and \hmmclient, the topics of most interest to end users.  This is followed by a discussion of the issues involved in converting an instance of \hmmpgmd to \hmmserver, and then a discussion of the design and implementation of \hmmserver.



\chapter{Installation}
\Hmmserver and \hmmclient are included in the standard HMMER distribution package, but HMMER must be compiled with MPI support turned on for \hmmserver to be useful, so it is likely you will have to compile HMMER from source to use the server.  To do this, obtain a source-code copy of HMMER (see the \userguide for instructions on how to do this) and go through the standard configuration/build process, with one change: you must pass the \mono{--enable-mpi} flag to our configure script to cause HMMER to be built with MPI support:

\vspace{1ex}
\user{\% ./configure {-}{-}prefix=/your/install/path {-}{-}enable-mpi}\\
\user{\% make}
\vspace{1ex}

\Hmmclient does not require MPI support, as it is a single-threaded program that communicates with a server via sockets.  If you only want to use \hmmclient to send searches to an existing server, any installation of HMMER, including ones from package managers or Linux distributions, should provide a working version of \hmmclient.

\chapter{Usage}

\section{Hmmserver}

\section{Hmmclient}

\chapter{For those converting from an hmmpgmd installation}

\chapter{Hmmserver's Design}
\Hmmserver is an MPI application that uses a conventional master-worker topology.  While the most-common use of \hmmserver will distribute the MPI ranks across multiple machines, this is not required\sidenote{If each MPI rank is not assigned to a separate computer, we strongly recommend using the \mono{--cpu} flag to specify the number of worker threads on each worker rank.  If this is not done, each worker rank will start as many worker threads as its machine has hardware thread slots, severely over-subscribing the hardware if multiple worker ranks run on the same machine.}.  One MPI rank (rank 0) acts as the master rank, while the rest act as workers.  The master rank handles communication with clients, receiving search requests from them and sending results back when each search completes.  The master rank also co-ordinates the work of the workers, assigning them regions of the target database to search and collecting the hits that each worker finds into a single list.  Worker ranks cache the server's target databases in memory and perform the work of each search.  Thus, the worker ranks can require significant memory, depending on the number and size of the target databases and the number of shards those databases are divided into.  The master rank does not cache target databases, as it only needs to know the number of items in each database to distribute work across the worker ranks.  Thus, it uses very little memory when the server is idle, but it may require significant amounts of memory to assemble the results of searches that return large numbers of hits.

The decision to build \hmmserver as an MPI application was a significant change from \hmmpgmd, which was implemented as a set of independent programs that communicate with each other via UNIX sockets.  The initial motivation for this change was to improve performance by taking advantage of high-bandwidth networks, which MPI typically does and sockets typically does not.  During discussions with our collaborators at the European Bioinformatics Institute, they disclosed that managing \hmmpgmd's independent programs was a source of difficulty for them, so the switch to an MPI application provided a second benefit.

\section{Parallelization and Load Balancing}
\Hmmpgmd statically distributed the work of each search across the worker nodes, assigning $\frac{1}{Nth}$ of the items to be searched to each worker node in a system with N workers.  This led to significant variance in the amount of time each worker took to perform its fraction of the search, reducing search performance. The root cause of this load imbalance is the fact that the amount of time to perform a sequence-HMM comparison varies greatly depending on whether the sequence and HMM are clearly not homologs, are close to being similar enough to be considered homologs, or are detected as homologs.  This causes regions of the database that contain many homologs to take much longer to search than regions that contain few homologs.  Thus, worker nodes that find many homologs take significantly longer to finish their fraction of a search than worker nodes that find few homologs.  Worse yet, the distribution of homologs across the target database varies from search to search, making it impossible to find a static allocation of work to worker nodes that balances load well.

To address this, \hmmserver dynamically distributes work across its worker node using a work-requesting system.  The master node maintains a global list of the items in each shard that still need to be compared to complete the search.  Worker nodes each maintain a local list of work (items to be searched) that they are responsible for.  When the amount of work remaining on a worker node's local list drops below a threshold, it sends a request to the master node for more work.  If the master node has any work remaining on its global list, it responds with a chunk of work (range of items) that the worker node is now responsible for.  To trade-off load balancing against the amount of communication required, work chunks contain a large amount of work at the beginning of a search and decrease in size over the course of the search.

\subsection{Within-Node Parallelization}
Within each worker node, \hmmpgmd uses a load-balancing system similar to how \hmmserver distributes work across nodes.  Each worker node maintains a list of the work assigned to it, and worker threads request chunks of work from the list as they complete their previously-assigned work.  This creates a trade-off between load balancing and contention for the work list: if the work chunks are too small, the worker threads need to request more work frequently, and access to the work list becomes a bottleneck.  On the other hand, if the work chunks are too large, the worker thread that grabs the last chunk of work from the list can wind up working for a significant amount of time after the other threads finish, increasing the time to complete each search.

\Hmmserver improves load-balancing by adding work stealing to \hmmpgmd's within-node load balancing scheme.  When a worker thread finds that the work list is empty, it examines the chunks of work that the other worker threads are working on and takes half of the work from the thread with the most to do.  This spreads the work at the end of a search more-evenly across the worker threads, preventing one thread from significantly delaying the end of the search.

\Hmmserver also adds split-phase processing of sequence-HMM comparisons to further improve performance.  The final steps of hit and domain detection can take a significant amount of time, such that the threads processing the last few hits in a search can delay completion noticeably, particularly when running the server on a large enough number of machines to meet our one second average search time goal\sidenote{This was definitely true in the HMMER4 performance prototype of the server, as HMMER4's final stage trades run time for accuracy in some cases.  We have not done a great deal of analysis on whether it is true in HMMER3.}.  To address this, the server splits sequence-HMM comparisons into "front-end" (the filter pipeline) and "back-end" (the main stage) processing.  Initially, all of the worker threads start processing the front end of comparisons.  When a comparison passes all of the steps in the front end, it is placed in a queue for back-end processing, and one of the worker threads is switched to process comparisons out of the back-end queue.  If the depth of the back-end queue exceeds a threshold, additional worker threads are switched to processing the back end of comparisons.  Similarly, if the back-end queue drops below a depth threshold, worker threads are switched back to processing the front end of comparisons.  This has the effect of prioritizing the back-end comparisons that take the longest to handle, decreasing the chance that a small number of long-running comparisons extend the overall search time.


\section{Sharding}


\section{Client-Server Interface}

\begin{adjustwidth}{}{-1in}   
\chapter{Manual Pages Related to the Server}
\input{manpages_server}
\end{adjustwidth}

\chapter{Acknowledgments}
Simon Potter of the European Bioinformatics Institute was of great help in understanding the daemon's interactions with the EBI's web servers.  We would also like to thank all of the organizations that have supported the development of HMMER, as well as all of the individuals who have contributed to it. In particular, Washington University, the National Institutes of Health, Monsanto, the Howard Hughes Medical Institute, and Harvard University have been major supporters of this work.  For a more thorough set of acknowledgments that includes a discussion of HMMER's history, please see the \underline{HMMER User's Guide}.

\label{manualend}

% To create distributable/gitted 'distilled.bib' from lab's bibtex dbs:
%   # uncomment the {master,lab,books};
%   pdflatex main
%   bibdistill main.aux > distilled.bib
%   # restore the {distilled} 
% 
\nobibliography{distilled}
%\nobibliography{master,lab,books}

\end{document}



